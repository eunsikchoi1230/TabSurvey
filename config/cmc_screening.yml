# General parameters
dataset: CMCscreening
problem_transformation: BinaryRelevance # BinaryRelevance, ClassifierChain, LabelPowerset, None
model_name: NaiveBayes # NaiveBayes, LogisticRegression, KNN, SVM, RandomForest, XGBoost, (CatBoost, LightGBM), DNN, TabNet
objective: multi-label_classification # Don't change
optimize_hyperparameters: True

# GPU parameters
use_gpu: True
gpu_ids: [0, 1]
data_parallel: True

# Optuna parameters - https://optuna.org/
n_trials: 20
n_startup_trials: 10
direction: minimize

# Cross validation parameters 
num_splits: 3
shuffle: True
seed: 221 # Don't change 

# Test train valid split parameters
test_size: 0.2
valid_size: 0.2
## Need to implement random, labelset, iterative (not implemented yet) stratification methods

# Preprocessing parameters
fill_na: False
scale: True
target_encode: False # Need implementation for multi-label classification
one_hot_encode: False

# Training parameters
batch_size: 1024
val_batch_size: 1024
early_stopping_rounds: 20
epochs: 1000
logging_period: 100

# About the data
num_classes: 4  # Number of labels
num_features: 41
cat_idx: [0, 13, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38] # Index of categorical features after dropping unused features
## cat_dims: will be automatically set.
# cat_dims: [] # Number of categories in each categorical feature